{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Tutorial 2: Linear Models\n",
    "Today we will learn how to implement Logistic and Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression\n",
    "Logistic regression is a supervised learning classification algorithm, which means it can estimate the class of new observation based on labeled observations. In essence, logistic regression models the probability that an observation belongs to a particular category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn how to implement Logistic Regression from scratch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dataset\n",
    "We will apply logistic regression to a binary classification problem with 10 observations and 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(\n",
    "        n_samples=10, \n",
    "        n_features=2,\n",
    "        n_redundant=0,\n",
    "        n_informative=2,\n",
    "        random_state=1, \n",
    "        n_clusters_per_class=1)\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "We have 10 observations $X$ with 2 features $\\theta$ and we know their labels $y$. Can we predict the label of a new observation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the Hypothesis Function\n",
    "Logistic regression uses the $\\textbf{Sigmoid Function}$ to calculate probability of an observation belonging to a particular class. The function returns a value between 0 and 1. \n",
    "The Sigmoid function is defined as:\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our hypothesis as follows: \n",
    "$$\n",
    "h(X) = g(X\\theta)\n",
    "$$\n",
    "$$\n",
    "h(x) = \\frac{1}{1+ e^{-X\\theta}}\n",
    "$$\n",
    "\n",
    "$h(X)$ gives us the probability of our output being 1, that is\n",
    "* if $X\\theta \\geq 0$, then $h(X) \\geq 0.5$, then predict $y=1$\n",
    "* if $X\\theta < 0$, then $h(X) < 0.5$, then predict $y=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the instructions to complete the two functions below.\n",
    "1. $\\texttt{sigmoidFn}$: Takes a parameter $z$, and returns the sigmoid of $z$. \n",
    "2. $\\texttt{hypothesis}$: Takes parameters $X$, $\\theta$ and returns $h(x)$. Define $z$ in this function as $X\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoidFn(z):\n",
    "    return TODO\n",
    "\n",
    "def hypothesis(X, theta):\n",
    "    z = TODO\n",
    "    h = TODO\n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "First, we will add an intercept to our $X$ values and initialize $\\theta$ to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.64185521  0.78535215]\n",
      " [ 1.          0.10972634  1.83671408]\n",
      " [ 1.         -1.07362634  2.591487  ]\n",
      " [ 1.         -1.38222372  0.85340843]\n",
      " [ 1.          1.78689446  2.3841826 ]\n",
      " [ 1.          0.94785273  3.53015683]\n",
      " [ 1.          1.08876018  1.35925144]\n",
      " [ 1.          0.42774158  3.54015499]\n",
      " [ 1.         -0.88398194 -0.57876171]\n",
      " [ 1.         -1.59347411  1.2168913 ]]\n",
      "[0. 0. 0.]\n",
      "[0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "def add_intercept(X):\n",
    "    intercept = np.ones((X.shape[0], 1))\n",
    "    return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "XX = add_intercept(X)\n",
    "#initialize theta to zero with shape equal to number of features\n",
    "theta = TODO\n",
    "h = hypothesis(XX, theta)\n",
    "\n",
    "print(XX)\n",
    "print(theta)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "The cost function is defined as follows:\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m \\Big( y_i \\text{log}(h_i) + (1-y_i)\\text{log}(1-h_i) \\Big)\n",
    "$$\n",
    "\n",
    "where $m$ is the number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "#implement cost function here\n",
    "cost = TODO\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "In order to get an accurate prediction, we need to minimise the cost function. This is done using Gradient Descent where we take the derivative of the cost function w.r.t $\\theta$. The overall functionality is given below.\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta}\n",
    "$$\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "We can show that,\n",
    "$$\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta} = \\frac{1}{m} X^T (h-y)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = y.size\n",
    "alpha = 0.01\n",
    "\n",
    "#compute gradient here\n",
    "gradient = TODO\n",
    "#update theta here\n",
    "theta = TODO\n",
    "\n",
    "print(gradient)\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "Now we will repeat the step to calculate the gradient and theta for a number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 100000\n",
    "cost_list = []\n",
    "\n",
    "for i in range(num_iter):\n",
    "    h = hypothesis(XX, theta)\n",
    "    cost = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    cost_list.append(cost)\n",
    "    \n",
    "    gradient = np.dot(XX.T, (h - y)) / m\n",
    "    theta -= alpha * gradient\n",
    "\n",
    "\n",
    "    if i % 10000 == 0:\n",
    "        print('Cost: {}'.format(cost))\n",
    "\n",
    "print('Adjusted coefficient: {}'.format(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_iter), cost_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicted Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_prob = hypothesis(XX, theta)\n",
    "preds = preds_prob.round()\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x = np.array([1, -0.8, 0.8])   # new observation (-0.8, 0.8)  with an intercept of 1\n",
    "print(new_x)\n",
    "preds_prob_new_x = hypothesis(new_x, theta).round()\n",
    "print(\"predicted output for the new observation: \", preds_prob_new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Boundary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# define 2d grid\n",
    "x1_min, x1_max = X[:,0].min(), X[:,0].max(),\n",
    "x2_min, x2_max = X[:,1].min(), X[:,1].max(),\n",
    "xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))\n",
    "grid = np.c_[xx1.ravel(), xx2.ravel()]\n",
    "\n",
    "# make predictions on the grid\n",
    "grid = add_intercept(grid)\n",
    "probs = hypothesis(grid, theta)\n",
    "probs = probs.reshape(xx1.shape)\n",
    "\n",
    "# plot contours\n",
    "ax = plt.gca()\n",
    "plt.contourf(xx1, xx2, probs, levels=25, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black') # decision boundary at 0.5\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), s=40, cmap=plt.cm.Spectral, edgecolors='black')\n",
    "plt.plot(-0.8, 0.8, 'bx', markersize=20, markeredgewidth=2)  # new observation correctly classified as 1 (blue)\n",
    "\n",
    "plt.xlabel(\"$X_1$\")\n",
    "plt.ylabel(\"$X_2$\")\n",
    "ax.set_xlim([x1_min, x1_max])\n",
    "ax.set_ylim([x2_min, x2_max])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
